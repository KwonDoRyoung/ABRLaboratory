{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KwonDoRyoung/ABRLaboratory/blob/main/0722/3_train_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny24LZUolz1p"
      },
      "outputs": [],
      "source": [
        "# import transforms and datasets in TorchVision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# load or download MNIST datasets\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=T.ToTensor())\n",
        "valid_data = datasets.MNIST(root='./data', train=True, download=True, transform=T.ToTensor())"
      ],
      "id": "ny24LZUolz1p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc0qlc9Tlz1r",
        "outputId": "db21350a-a8d5-4334-bb9a-2b45cfd84e56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa200182a70>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# setup configuration\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "valid_ratio = 0.1\n",
        "shuffle = True\n",
        "random_seed = 0xAB\n",
        "\n",
        "# fix random seed\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "#torch.backends.cudnn.deterministic = True\n",
        "#torch.backends.cudnn.benchmark = False"
      ],
      "id": "Nc0qlc9Tlz1r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDA03ZjLlz1t"
      },
      "outputs": [],
      "source": [
        "# split given train data into train and valid dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "num_valid = int(valid_ratio * num_train)\n",
        "\n",
        "if shuffle:\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[num_valid:], indices[:num_valid]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)"
      ],
      "id": "DDA03ZjLlz1t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KIbW51blz1t"
      },
      "outputs": [],
      "source": [
        "# import DataLoader in PyTorch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# create data loaders to feed data into our model\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_dataloader = DataLoader(valid_data, batch_size=batch_size, sampler=valid_sampler)"
      ],
      "id": "-KIbW51blz1t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfQcBWA2lz1t"
      },
      "outputs": [],
      "source": [
        "# import nn in PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# define MLP network with one hidden layer (original version)\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.flatten = nn.Flatten() # flatten the input tensor as a 1D vector ((28, 28) -> (784))\n",
        "        self.input_layer = nn.Linear(28*28, 512)\n",
        "        self.hidden_layer = nn.Linear(512, 256)\n",
        "        self.output_layer = nn.Linear(256, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        h = self.relu(self.input_layer(x))\n",
        "        h = self.relu(self.hidden_layer(h))\n",
        "        y = self.output_layer(h)\n",
        "        return y"
      ],
      "id": "mfQcBWA2lz1t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "EfVsUbQ5lz1u"
      },
      "outputs": [],
      "source": [
        "# import cuda in PyTorch\n",
        "import torch.cuda as cuda\n",
        "\n",
        "# check device for training\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# get the MLP model and send it to the device\n",
        "myMLP = MLP().to(device)\n",
        "\n",
        "# define a loss function and an optimizer\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='mean') # cross entropy loss function for classification\n",
        "optimizer = torch.optim.SGD(myMLP.parameters(), lr=5e-3) # stochastic gradient descent with learning rate of 0.005"
      ],
      "id": "EfVsUbQ5lz1u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vdnmZeVlz1u",
        "outputId": "63fcac5a-2657-4ca0-c82e-2a86d160a6d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training starts!\n",
            "\n",
            "epoch 1\n",
            "------------------------------\n",
            "[    0/60000]  train loss: 2.307188  valid accuracy: 6.5%, valid loss: 2.307819\n",
            "[ 6400/60000]  train loss: 2.282454  valid accuracy: 19.5%, valid loss: 2.284902\n",
            "[12800/60000]  train loss: 2.273978  valid accuracy: 38.3%, valid loss: 2.261636\n",
            "[19200/60000]  train loss: 2.222906  valid accuracy: 45.3%, valid loss: 2.233253\n",
            "[25600/60000]  train loss: 2.202410  valid accuracy: 56.7%, valid loss: 2.196545\n",
            "[32000/60000]  train loss: 2.135819  valid accuracy: 67.2%, valid loss: 2.148727\n",
            "[38400/60000]  train loss: 2.090995  valid accuracy: 70.0%, valid loss: 2.085641\n",
            "[44800/60000]  train loss: 1.986894  valid accuracy: 70.8%, valid loss: 2.000939\n",
            "[51200/60000]  train loss: 1.915801  valid accuracy: 73.2%, valid loss: 1.893889\n",
            "\n",
            "epoch 2\n",
            "------------------------------\n",
            "[    0/60000]  train loss: 1.844471  valid accuracy: 73.6%, valid loss: 1.836896\n",
            "[ 6400/60000]  train loss: 1.642508  valid accuracy: 74.2%, valid loss: 1.689183\n",
            "[12800/60000]  train loss: 1.593756  valid accuracy: 75.0%, valid loss: 1.523983\n",
            "[19200/60000]  train loss: 1.420146  valid accuracy: 75.2%, valid loss: 1.359760\n",
            "[25600/60000]  train loss: 1.279173  valid accuracy: 77.0%, valid loss: 1.206556\n",
            "[32000/60000]  train loss: 1.129773  valid accuracy: 77.7%, valid loss: 1.076740\n",
            "[38400/60000]  train loss: 1.011421  valid accuracy: 79.1%, valid loss: 0.970486\n",
            "[44800/60000]  train loss: 0.936011  valid accuracy: 80.5%, valid loss: 0.884719\n",
            "[51200/60000]  train loss: 0.716851  valid accuracy: 81.1%, valid loss: 0.816853\n",
            "\n",
            "epoch 3\n",
            "------------------------------\n",
            "[    0/60000]  train loss: 0.765249  valid accuracy: 80.9%, valid loss: 0.791429\n",
            "[ 6400/60000]  train loss: 0.893074  valid accuracy: 82.2%, valid loss: 0.739100\n",
            "[12800/60000]  train loss: 0.814156  valid accuracy: 83.3%, valid loss: 0.695716\n",
            "[19200/60000]  train loss: 0.662160  valid accuracy: 84.2%, valid loss: 0.661133\n",
            "[25600/60000]  train loss: 0.618060  valid accuracy: 84.4%, valid loss: 0.629368\n",
            "[32000/60000]  train loss: 0.500040  valid accuracy: 84.8%, valid loss: 0.603618\n",
            "[38400/60000]  train loss: 0.619408  valid accuracy: 85.4%, valid loss: 0.580221\n",
            "[44800/60000]  train loss: 0.551215  valid accuracy: 85.5%, valid loss: 0.560498\n",
            "[51200/60000]  train loss: 0.548252  valid accuracy: 86.0%, valid loss: 0.541505\n",
            "\n",
            "epoch 4\n",
            "------------------------------\n",
            "[    0/60000]  train loss: 0.644022  valid accuracy: 86.1%, valid loss: 0.534364\n",
            "[ 6400/60000]  train loss: 0.508241  valid accuracy: 86.5%, valid loss: 0.518739\n",
            "[12800/60000]  train loss: 0.557370  valid accuracy: 86.7%, valid loss: 0.507617\n",
            "[19200/60000]  train loss: 0.663920  valid accuracy: 87.0%, valid loss: 0.495319\n",
            "[25600/60000]  train loss: 0.449372  valid accuracy: 87.3%, valid loss: 0.482073\n",
            "[32000/60000]  train loss: 0.452387  valid accuracy: 87.4%, valid loss: 0.471910\n",
            "[38400/60000]  train loss: 0.625510  valid accuracy: 87.7%, valid loss: 0.463299\n",
            "[44800/60000]  train loss: 0.526793  valid accuracy: 87.9%, valid loss: 0.452567\n",
            "[51200/60000]  train loss: 0.568083  valid accuracy: 88.3%, valid loss: 0.445903\n",
            "\n",
            "epoch 5\n",
            "------------------------------\n",
            "[    0/60000]  train loss: 0.263715  valid accuracy: 88.0%, valid loss: 0.443623\n",
            "[ 6400/60000]  train loss: 0.366178  valid accuracy: 88.1%, valid loss: 0.436024\n",
            "[12800/60000]  train loss: 0.477142  valid accuracy: 88.5%, valid loss: 0.430627\n",
            "[19200/60000]  train loss: 0.321332  valid accuracy: 88.5%, valid loss: 0.423216\n",
            "[25600/60000]  train loss: 0.372969  valid accuracy: 88.3%, valid loss: 0.418868\n",
            "[32000/60000]  train loss: 0.370574  valid accuracy: 88.9%, valid loss: 0.412450\n",
            "[38400/60000]  train loss: 0.448174  valid accuracy: 88.7%, valid loss: 0.410165\n",
            "[44800/60000]  train loss: 0.301532  valid accuracy: 89.1%, valid loss: 0.403586\n",
            "[51200/60000]  train loss: 0.244109  valid accuracy: 89.1%, valid loss: 0.397131\n",
            "\n",
            "training is finished!\n"
          ]
        }
      ],
      "source": [
        "# train the MLP model\n",
        "epochs = 5\n",
        "report_interval = 100\n",
        "max_valid_acc = 0\n",
        "save_path = 'model.pt'\n",
        "\n",
        "ndata = len(train_dataloader.dataset) \n",
        "print('training starts!')\n",
        "\n",
        "for e in range(epochs):\n",
        "    print(f'\\nepoch {e+1}\\n------------------------------')\n",
        "    myMLP.train() # train mode\n",
        "    \n",
        "    for b, (X, y) in enumerate(train_dataloader):\n",
        "        X, y = X.to(device), y.to(device) # input and target to device(gpu)\n",
        "\n",
        "        prediction = myMLP(X) # forward pass\n",
        "        train_loss = loss_fn(prediction, y) # calculate the loss \n",
        "\n",
        "        optimizer.zero_grad() # clear gradients\n",
        "        train_loss.backward() # backpropagation\n",
        "        optimizer.step() # update the parameters\n",
        "\n",
        "        if b % report_interval == 0: # track the training\n",
        "            train_loss, current = train_loss.item(), b * len(X) \n",
        "            print(f'[{current:>5d}/{ndata:>5d}]  train loss: {train_loss:>7f}  ', end=\"\")\n",
        "            \n",
        "            myMLP.eval()\n",
        "            with torch.no_grad():\n",
        "                valid_ndata = 0\n",
        "                valid_nbatch = len(valid_dataloader)\n",
        "                valid_loss, valid_correct = 0, 0\n",
        "                for X, y in valid_dataloader:\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    prediction = myMLP(X)\n",
        "                    valid_loss += loss_fn(prediction, y).item() # add up the loss\n",
        "                    valid_correct += (prediction.argmax(1) == y).type(torch.float).sum().item() # add up the correct predictions\n",
        "                    valid_ndata += len(X)\n",
        "                valid_loss /= valid_nbatch\n",
        "                valid_correct /= valid_ndata\n",
        "                print(f\"valid accuracy: {(100*valid_correct):>0.1f}%, valid loss: {valid_loss:>8f}\")\n",
        "            \n",
        "            if max_valid_acc < valid_correct:\n",
        "                torch.save(myMLP.state_dict(), save_path)\n",
        "                max_valid_acc = valid_correct\n",
        "\n",
        "print('\\ntraining is finished!')"
      ],
      "id": "9vdnmZeVlz1u"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vaca",
      "language": "python",
      "name": "vaca"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "name": "3-train_mnist.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}