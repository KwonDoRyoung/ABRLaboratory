{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2380e12-1327-40d0-b793-2839c79e1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c444fc73-e390-4721-983e-6679a25fb2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a7ae345c8148d7b6ef370a2b8a79f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03554e47ea74be39032e08d62d1b912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e79b1edd9ba4b7295a3e65418bbdb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6e87cfed88424d9afd7c188ed9c234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load or download MNIST datasets\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.7,), (0.7,)),]) # images to tensors with normalization\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=trans)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fde4fd-18cb-41a1-8493-186a5f487b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1915fd-9c18-42ee-bcd0-171c466e2da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [batch, channel, height, width]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) -> (torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# check the data from the data loader\n",
    "for X, y in test_dataloader:\n",
    "    print(f'Shape of X [batch, channel, height, width]: {X.shape}')\n",
    "    print(f'Shape of y: {y.shape} -> ({y.dtype})')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05771554-f1a5-4b13-b087-43845409c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MLP network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input_layer = nn.Linear(28*28, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.hidden_layer = nn.Linear(256, 512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(512, 10)\n",
    "        # self.linear_relu_layers = nn.Sequential(nn.Linear(28*28, 256),\n",
    "        #                                         nn.ReLU(),\n",
    "        #                                         nn.Linear(256, 512),\n",
    "        #                                         nn.ReLU(),\n",
    "        #                                         nn.Linear(512, 10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        h = self.relu1(self.input_layer(x))\n",
    "        h = self.relu2(self.hidden_layer(h))\n",
    "        y = self.output_layer(h)\n",
    "        # y = self.linear_relu_layers(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05771554-f1a5-4b13-b087-43845409c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MLP network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_layers = nn.Sequential(nn.Linear(28*28, 256),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(256, 512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(512, 10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_layers(self.flatten(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42408b17-374c-48ce-bea3-60b1750e882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check device for training\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437faf76-38d9-4216-8964-ffa0eb41a823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (input_layer): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (hidden_layer): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (output_layer): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# get a MLP model and send it to device\n",
    "myMLP = MLP().to(device)\n",
    "print(myMLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672f3c57-45fc-4662-86e8-a5183473ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a loss function and an optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(myMLP.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eceae1bc-2cbc-4bc5-b730-e8d3dc5bba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts!\n",
      "\n",
      "epoch 1\n",
      "------------------------------\n",
      "loss: 0.258528  [    0/60000]\n",
      "loss: 0.273169  [ 6400/60000]\n",
      "loss: 0.259676  [12800/60000]\n",
      "loss: 0.361321  [19200/60000]\n",
      "loss: 0.263151  [25600/60000]\n",
      "loss: 0.337552  [32000/60000]\n",
      "loss: 0.229728  [38400/60000]\n",
      "loss: 0.393709  [44800/60000]\n",
      "loss: 0.358780  [51200/60000]\n",
      "loss: 0.423601  [57600/60000]\n",
      "\n",
      "epoch 2\n",
      "------------------------------\n",
      "loss: 0.232001  [    0/60000]\n",
      "loss: 0.266170  [ 6400/60000]\n",
      "loss: 0.243248  [12800/60000]\n",
      "loss: 0.349420  [19200/60000]\n",
      "loss: 0.245207  [25600/60000]\n",
      "loss: 0.325074  [32000/60000]\n",
      "loss: 0.218572  [38400/60000]\n",
      "loss: 0.379761  [44800/60000]\n",
      "loss: 0.337337  [51200/60000]\n",
      "loss: 0.414267  [57600/60000]\n",
      "\n",
      "epoch 3\n",
      "------------------------------\n",
      "loss: 0.210979  [    0/60000]\n",
      "loss: 0.260296  [ 6400/60000]\n",
      "loss: 0.231741  [12800/60000]\n",
      "loss: 0.338896  [19200/60000]\n",
      "loss: 0.231851  [25600/60000]\n",
      "loss: 0.312602  [32000/60000]\n",
      "loss: 0.209266  [38400/60000]\n",
      "loss: 0.371084  [44800/60000]\n",
      "loss: 0.319284  [51200/60000]\n",
      "loss: 0.405554  [57600/60000]\n",
      "\n",
      "epoch 4\n",
      "------------------------------\n",
      "loss: 0.194815  [    0/60000]\n",
      "loss: 0.257397  [ 6400/60000]\n",
      "loss: 0.222274  [12800/60000]\n",
      "loss: 0.327625  [19200/60000]\n",
      "loss: 0.221650  [25600/60000]\n",
      "loss: 0.301671  [32000/60000]\n",
      "loss: 0.200686  [38400/60000]\n",
      "loss: 0.363937  [44800/60000]\n",
      "loss: 0.302609  [51200/60000]\n",
      "loss: 0.397347  [57600/60000]\n",
      "\n",
      "epoch 5\n",
      "------------------------------\n",
      "loss: 0.180789  [    0/60000]\n",
      "loss: 0.253582  [ 6400/60000]\n",
      "loss: 0.213326  [12800/60000]\n",
      "loss: 0.318244  [19200/60000]\n",
      "loss: 0.212783  [25600/60000]\n",
      "loss: 0.292418  [32000/60000]\n",
      "loss: 0.192803  [38400/60000]\n",
      "loss: 0.355958  [44800/60000]\n",
      "loss: 0.284760  [51200/60000]\n",
      "loss: 0.390451  [57600/60000]\n",
      "-------------------------\n",
      "training is finished!\n"
     ]
    }
   ],
   "source": [
    "# train the MLP model\n",
    "epochs = 5\n",
    "ndata = len(train_dataloader.dataset)\n",
    "print('training starts!')\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f'\\nepoch {e+1}\\n------------------------------')\n",
    "    myMLP.train() # train mode\n",
    "    \n",
    "    for nbatch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device) # tensors to device(gpu)\n",
    "\n",
    "        prediction = myMLP(X)\n",
    "        train_loss = loss_fn(prediction, y) # calculate the loss of the prediction on the batch\n",
    "\n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        train_loss.backward() # backpropagation\n",
    "        optimizer.step()\n",
    "\n",
    "        if nbatch % 100 == 0: # track the training\n",
    "            train_loss, current = train_loss.item(), nbatch * len(X)\n",
    "            print(f'loss: {train_loss:>7f}  [{current:>5d}/{ndata:>5d}]')\n",
    "print('\\ntraining is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf328f21-3762-4071-8c02-6d1180731269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error\n",
      "-> accuracy: 90.5%, average loss: 0.324948\n"
     ]
    }
   ],
   "source": [
    "# test the MLP model\n",
    "ndata = len(test_dataloader.dataset)\n",
    "nbatch = len(test_dataloader)\n",
    "myMLP.eval() # test mode\n",
    "test_loss, correct = 0, 0 # initialization\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        prediction = myMLP(X)\n",
    "        test_loss += loss_fn(prediction, y).item()\n",
    "        correct += (prediction.argmax(1) == y).type(torch.float).sum().item() # calculate and accumulate the loss\n",
    "test_loss /= nbatch # average loss for all batches\n",
    "correct /= ndata # accuracy for all data\n",
    "print(f\"test error\\n-> accuracy: {(100*correct):>0.1f}%, average loss: {test_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571f1fd-ccb7-41b1-946f-88508cbe271f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
