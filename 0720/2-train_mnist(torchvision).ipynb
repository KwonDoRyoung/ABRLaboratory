{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9719a6-904f-45df-bf66-af4c076a086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (comment) import necessary modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0b7c6d-2873-42b4-96e2-94e65a97301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load or download MNIST datasets\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.7,), (0.7,)),]) # transform image data to tensors and normalize them (mean=0.7, std=0.7)\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=trans)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25ab270-3ba7-408e-b0ac-5bc87c0c83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b000166f-36c7-4dd9-a4cf-822830e2dac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [batch, channel, height, width]: torch.Size([64, 1, 28, 28]) (torch.float32)\n",
      "Shape of y: torch.Size([64]) (torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# check the data from the data loader\n",
    "for X, y in train_dataloader:\n",
    "    print(f'Shape of X [batch, channel, height, width]: {X.shape} ({X.dtype})') \n",
    "    print(f'Shape of y: {y.shape} ({y.dtype})')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c075ce58-710b-4fa9-9465-accfcd9ff8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MLP network with one hidden layer (original version)\n",
    "# run one of this or below cell\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten() # flatten the input tensor as a 1D vector ((28, 28) -> (784))\n",
    "        self.input_layer = nn.Linear(28*28, 512)\n",
    "        self.hidden_layer = nn.Linear(512, 256)\n",
    "        self.output_layer = nn.Linear(256, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        h = self.relu(self.input_layer(x))\n",
    "        h = self.relu(self.hidden_layer(h))\n",
    "        y = self.output_layer(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb2ba7e-2132-420a-9650-0aafadd53b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MLP network (simple version)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_layers = nn.Sequential(nn.Linear(28*28, 256),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(256, 512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(512, 10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_layers(self.flatten(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55111636-3fe9-45af-8ba4-b5659e4ac062",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check device for training\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0813820-abe0-43e3-9745-42a861683540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# get the MLP model and send it to the device\n",
    "myMLP = MLP().to(device)\n",
    "print(myMLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9bf45c-171f-4fb8-851a-342e6ababfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define a loss function and an optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # loss function for classification\n",
    "optimizer = torch.optim.SGD(myMLP.parameters(), lr=5e-3) # stochastic gradient descent with learning rate of 0.005\n",
    "print(loss_fn)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1a2129-2521-463c-8d86-a1031f7658f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts!\n",
      "\n",
      "epoch 1\n",
      "------------------------------\n",
      "loss: 2.291759  [    0/60000]\n",
      "loss: 2.223994  [ 6400/60000]\n",
      "loss: 2.168183  [12800/60000]\n",
      "loss: 2.010625  [19200/60000]\n",
      "loss: 1.827311  [25600/60000]\n",
      "loss: 1.603575  [32000/60000]\n",
      "loss: 1.267062  [38400/60000]\n",
      "loss: 1.254340  [44800/60000]\n",
      "loss: 1.026646  [51200/60000]\n",
      "loss: 0.822613  [57600/60000]\n",
      "\n",
      "epoch 2\n",
      "------------------------------\n",
      "loss: 0.843861  [    0/60000]\n",
      "loss: 0.688846  [ 6400/60000]\n",
      "loss: 0.673070  [12800/60000]\n",
      "loss: 0.608711  [19200/60000]\n",
      "loss: 0.542259  [25600/60000]\n",
      "loss: 0.528805  [32000/60000]\n",
      "loss: 0.451171  [38400/60000]\n",
      "loss: 0.574159  [44800/60000]\n",
      "loss: 0.559926  [51200/60000]\n",
      "loss: 0.499349  [57600/60000]\n",
      "\n",
      "epoch 3\n",
      "------------------------------\n",
      "loss: 0.459933  [    0/60000]\n",
      "loss: 0.368058  [ 6400/60000]\n",
      "loss: 0.404581  [12800/60000]\n",
      "loss: 0.448154  [19200/60000]\n",
      "loss: 0.355247  [25600/60000]\n",
      "loss: 0.411089  [32000/60000]\n",
      "loss: 0.324254  [38400/60000]\n",
      "loss: 0.464898  [44800/60000]\n",
      "loss: 0.462106  [51200/60000]\n",
      "loss: 0.455400  [57600/60000]\n",
      "\n",
      "epoch 4\n",
      "------------------------------\n",
      "loss: 0.346642  [    0/60000]\n",
      "loss: 0.293885  [ 6400/60000]\n",
      "loss: 0.319025  [12800/60000]\n",
      "loss: 0.407433  [19200/60000]\n",
      "loss: 0.295952  [25600/60000]\n",
      "loss: 0.371504  [32000/60000]\n",
      "loss: 0.277662  [38400/60000]\n",
      "loss: 0.419723  [44800/60000]\n",
      "loss: 0.417885  [51200/60000]\n",
      "loss: 0.440945  [57600/60000]\n",
      "\n",
      "epoch 5\n",
      "------------------------------\n",
      "loss: 0.288166  [    0/60000]\n",
      "loss: 0.270973  [ 6400/60000]\n",
      "loss: 0.275092  [12800/60000]\n",
      "loss: 0.386219  [19200/60000]\n",
      "loss: 0.268167  [25600/60000]\n",
      "loss: 0.346719  [32000/60000]\n",
      "loss: 0.253392  [38400/60000]\n",
      "loss: 0.393869  [44800/60000]\n",
      "loss: 0.389218  [51200/60000]\n",
      "loss: 0.432977  [57600/60000]\n",
      "\n",
      "training is finished!\n"
     ]
    }
   ],
   "source": [
    "# train the MLP model\n",
    "epochs = 5\n",
    "ndata = len(train_dataloader.dataset) \n",
    "print('training starts!')\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f'\\nepoch {e+1}\\n------------------------------')\n",
    "    myMLP.train() # train mode\n",
    "    \n",
    "    for b, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device) # input and target to device(gpu)\n",
    "\n",
    "        prediction = myMLP(X) # forward pass\n",
    "        train_loss = loss_fn(prediction, y) # calculate the loss \n",
    "\n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        train_loss.backward() # backpropagation\n",
    "        optimizer.step() # update the parameters\n",
    "\n",
    "        if b % 100 == 0: # track the training\n",
    "            train_loss, current = train_loss.item(), b * len(X) \n",
    "            print(f'loss: {train_loss:>7f}  [{current:>5d}/{ndata:>5d}]')\n",
    "print('\\ntraining is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b57ed3df-b201-4018-bd1f-2dbaf97b9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error\n",
      "-> accuracy: 90.6%, average loss: 0.320947\n"
     ]
    }
   ],
   "source": [
    "# test the MLP model\n",
    "ndata = len(test_dataloader.dataset)\n",
    "nbatch = len(test_dataloader)\n",
    "myMLP.eval() # test mode\n",
    "test_loss, correct = 0, 0 # initialize\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        prediction = myMLP(X)\n",
    "        test_loss += loss_fn(prediction, y).item() # add up the loss\n",
    "        correct += (prediction.argmax(1) == y).type(torch.float).sum().item() # add up the correct predictions\n",
    "test_loss /= nbatch # average the loss\n",
    "correct /= ndata # accuracy for all data\n",
    "print(f\"test error\\n-> accuracy: {(100*correct):>0.1f}%, average loss: {test_loss:>8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
