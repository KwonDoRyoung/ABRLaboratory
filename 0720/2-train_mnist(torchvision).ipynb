{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0b7c6d-2873-42b4-96e2-94e65a97301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transforms and datasets in TorchVision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# load or download MNIST datasets\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=T.ToTensor())\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25ab270-3ba7-408e-b0ac-5bc87c0c83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DataLoader in PyTorch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create data loaders to feed data into our model\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b000166f-36c7-4dd9-a4cf-822830e2dac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [batch, channel, height, width]: torch.Size([64, 1, 28, 28]) (torch.float32)\n",
      "Shape of y: torch.Size([64]) (torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# check the data from the data loader\n",
    "for X, y in train_dataloader:\n",
    "    print(f'Shape of X [batch, channel, height, width]: {X.shape} ({X.dtype})') \n",
    "    print(f'Shape of y: {y.shape} ({y.dtype})')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c075ce58-710b-4fa9-9465-accfcd9ff8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nn in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define MLP network with one hidden layer (original version)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten() # flatten the input tensor as a 1D vector ((28, 28) -> (784))\n",
    "        self.input_layer = nn.Linear(28*28, 512)\n",
    "        self.hidden_layer = nn.Linear(512, 256)\n",
    "        self.output_layer = nn.Linear(256, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        h = self.relu(self.input_layer(x))\n",
    "        h = self.relu(self.hidden_layer(h))\n",
    "        y = self.output_layer(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55111636-3fe9-45af-8ba4-b5659e4ac062",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# import cuda in PyTorch\n",
    "import torch.cuda as cuda\n",
    "\n",
    "# check device for training\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0813820-abe0-43e3-9745-42a861683540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (input_layer): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (hidden_layer): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (output_layer): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# get the MLP model and send it to the device\n",
    "myMLP = MLP().to(device)\n",
    "print(myMLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9bf45c-171f-4fb8-851a-342e6ababfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define a loss function and an optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean') # cross entropy loss function for classification\n",
    "optimizer = torch.optim.SGD(myMLP.parameters(), lr=5e-3) # stochastic gradient descent with learning rate of 0.005\n",
    "\n",
    "print(loss_fn)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a1a2129-2521-463c-8d86-a1031f7658f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts!\n",
      "\n",
      "epoch 1\n",
      "------------------------------\n",
      "loss: 2.291039  [    0/60000]\n",
      "loss: 2.275304  [ 6400/60000]\n",
      "loss: 2.275133  [12800/60000]\n",
      "loss: 2.211379  [19200/60000]\n",
      "loss: 2.194322  [25600/60000]\n",
      "loss: 2.147560  [32000/60000]\n",
      "loss: 2.054236  [38400/60000]\n",
      "loss: 2.061951  [44800/60000]\n",
      "loss: 1.893292  [51200/60000]\n",
      "loss: 1.730148  [57600/60000]\n",
      "\n",
      "epoch 2\n",
      "------------------------------\n",
      "loss: 1.700732  [    0/60000]\n",
      "loss: 1.491445  [ 6400/60000]\n",
      "loss: 1.452919  [12800/60000]\n",
      "loss: 1.139206  [19200/60000]\n",
      "loss: 1.039339  [25600/60000]\n",
      "loss: 0.933566  [32000/60000]\n",
      "loss: 0.805866  [38400/60000]\n",
      "loss: 0.913249  [44800/60000]\n",
      "loss: 0.800716  [51200/60000]\n",
      "loss: 0.713659  [57600/60000]\n",
      "\n",
      "epoch 3\n",
      "------------------------------\n",
      "loss: 0.764488  [    0/60000]\n",
      "loss: 0.603892  [ 6400/60000]\n",
      "loss: 0.634781  [12800/60000]\n",
      "loss: 0.583562  [19200/60000]\n",
      "loss: 0.528263  [25600/60000]\n",
      "loss: 0.513650  [32000/60000]\n",
      "loss: 0.427318  [38400/60000]\n",
      "loss: 0.598129  [44800/60000]\n",
      "loss: 0.557169  [51200/60000]\n",
      "loss: 0.532391  [57600/60000]\n",
      "\n",
      "epoch 4\n",
      "------------------------------\n",
      "loss: 0.544864  [    0/60000]\n",
      "loss: 0.406587  [ 6400/60000]\n",
      "loss: 0.437062  [12800/60000]\n",
      "loss: 0.467752  [19200/60000]\n",
      "loss: 0.394319  [25600/60000]\n",
      "loss: 0.419658  [32000/60000]\n",
      "loss: 0.314814  [38400/60000]\n",
      "loss: 0.504369  [44800/60000]\n",
      "loss: 0.467826  [51200/60000]\n",
      "loss: 0.476816  [57600/60000]\n",
      "\n",
      "epoch 5\n",
      "------------------------------\n",
      "loss: 0.441744  [    0/60000]\n",
      "loss: 0.330494  [ 6400/60000]\n",
      "loss: 0.347654  [12800/60000]\n",
      "loss: 0.416281  [19200/60000]\n",
      "loss: 0.334538  [25600/60000]\n",
      "loss: 0.378739  [32000/60000]\n",
      "loss: 0.263662  [38400/60000]\n",
      "loss: 0.456684  [44800/60000]\n",
      "loss: 0.419716  [51200/60000]\n",
      "loss: 0.450907  [57600/60000]\n",
      "\n",
      "training is finished!\n"
     ]
    }
   ],
   "source": [
    "# train the MLP model\n",
    "epochs = 5\n",
    "ndata = len(train_dataloader.dataset) \n",
    "print('training starts!')\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f'\\nepoch {e+1}\\n------------------------------')\n",
    "    myMLP.train() # train mode\n",
    "    \n",
    "    for b, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device) # input and target to device(gpu)\n",
    "\n",
    "        prediction = myMLP(X) # forward pass\n",
    "        train_loss = loss_fn(prediction, y) # calculate the loss \n",
    "\n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        train_loss.backward() # backpropagation\n",
    "        optimizer.step() # update the parameters\n",
    "\n",
    "        if b % 100 == 0: # track the training\n",
    "            train_loss, current = train_loss.item(), b * len(X) \n",
    "            print(f'loss: {train_loss:>7f}  [{current:>5d}/{ndata:>5d}]')\n",
    "print('\\ntraining is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57ed3df-b201-4018-bd1f-2dbaf97b9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error\n",
      "-> accuracy: 89.9%, average loss: 0.354588\n"
     ]
    }
   ],
   "source": [
    "# test the MLP model\n",
    "ndata = len(test_dataloader.dataset)\n",
    "nbatch = len(test_dataloader)\n",
    "myMLP.eval() # test mode\n",
    "test_loss, correct = 0, 0 # initialize\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        prediction = myMLP(X)\n",
    "        test_loss += loss_fn(prediction, y).item() # add up the loss\n",
    "        test_ += (prediction.argmax(1) == y).type(torch.float).sum().item() # add up the correct predictions\n",
    "test_loss /= nbatch # average the loss\n",
    "correct /= ndata # accuracy for all data\n",
    "print(f\"test error\\n-> accuracy: {(100*correct):>0.1f}%, average loss: {test_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d760ed-d2ed-4604-abe4-892c1b370d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
